{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwrOl5FZxS36"
      },
      "source": [
        "# **⭕ Lung Tumor Detection And Segmentation Notebook**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ptFwYHCkFlC"
      },
      "source": [
        "### **✅ Problem Definition & Dataset Overview:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7n4dFMmyVwP"
      },
      "source": [
        "**`Dataset`**\\\n",
        "├── train\\\n",
        "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── detections\\\n",
        "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── images\\\n",
        "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── masks\\\n",
        "└── val\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── detections\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├── images\\\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└── masks\n",
        "- (detections, images, masks) in both trian & val should have subfolders one for each patient (subject).\n",
        "- Images and masks should be corresponding to each other and named the same.\n",
        "- Detections of each patient should include text files named with the scan number\n",
        "- and each scan file includes a line or more of 4 numbers each (xMin, yMin, xMax, yMax)\n",
        "\n",
        "**`Problem:`**\n",
        "Building **Recognition & Detection models** for lung-cancer tumors\n",
        "\n",
        "**`Solution:`**\\\n",
        "1- Apply and Train Yolo Object Detection Model.  \n",
        "2- Apply and Train UNet Segmentation Model for full images.  \n",
        "3- Apply and Train UNet Segmentation Model for cropped detection image.  \n",
        "4- Performance of valid data and Decide which technique is the best from 2 or 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BsjSRjlRfaXn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-16 11:53:44.632796: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-16 11:53:44.644911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1734342824.657769   34809 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1734342824.661267   34809 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-16 11:53:44.674826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import yaml\n",
        "from typing import List, Tuple\n",
        "from itertools import chain\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "%matplotlib inline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from IPython.display import display\n",
        "from seaborn import color_palette\n",
        "from data_loader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset consists of 256 x 256 grayscale images\n",
            "[ Loading Training Data   ]  Loaded [1832] Scans (1547 Cancer + 285 Healthy)\n",
            "[ Loading Validation Data ]  Loaded [98] Scans (78 Cancer + 20 Healthy)\n"
          ]
        }
      ],
      "source": [
        "# Global settings for plot styling\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "plt.rcParams['xtick.major.bottom'] = False\n",
        "plt.rcParams['ytick.major.left'] = False\n",
        "plt.rcParams['axes.grid'] = False\n",
        "\n",
        "data_loader = DataLoader('dataset/train', 'dataset/val')\n",
        "train, val = data_loader.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**`Sample Scans`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oK662psH1Q4G",
        "outputId": "826ea187-d39b-4667-e75f-cd2636c914ba"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(detections) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m detections:\n\u001b[0;32m---> 11\u001b[0m         xmin, ymin, xmax, ymax \u001b[38;5;241m=\u001b[39m det\n\u001b[1;32m     12\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(image, (xmin, ymin), (xmax, ymax), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m axes[row, col]\u001b[38;5;241m.\u001b[39mimshow(image)\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x1600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "random_scans = random.sample(list(train.values()), 16)\n",
        "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "for i, scan in enumerate(random_scans):\n",
        "    row = i // 4\n",
        "    col = i % 4\n",
        "\n",
        "    image = cv2.cvtColor(scan[0], cv2.COLOR_BGR2RGB)\n",
        "    detections = scan[2]\n",
        "    if len(detections) > 0:\n",
        "        for det in detections:\n",
        "            xmin, ymin, xmax, ymax = det\n",
        "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "    axes[row, col].imshow(image)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrz8H8tav4w3"
      },
      "source": [
        "# **⚡ Task1- Object Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC1mV9nkDnj2"
      },
      "source": [
        "### **✅ Training Yolo Model:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWzgIcw16mIS"
      },
      "source": [
        "**Yolo** is an algorithm that uses convolutional neural networks for object detection.\n",
        "Detection algorithm does not only predict class labels, but detects locations of objects as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmg1_GvjpcNs"
      },
      "source": [
        "**data.yaml** for Binary Classification You will only have two classes: one for images containing a tumor and one for images without a tumor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4vmcvyEo6dM",
        "outputId": "c043231c-476a-4e72-fc2f-55b9adf4532d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "⚠️ Download failure, retrying 1/3 https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt...\n",
            "curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 \n",
            "\n",
            "⚠️ Download failure, retrying 2/3 https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt...\n",
            "curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 \n",
            "\n",
            "⚠️ Download failure, retrying 3/3 https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt...\n",
            "curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443 \n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/utils/downloads.py\", line 337, in safe_download\n",
            "    assert r == 0, f\"Curl return value {r}\"\n",
            "AssertionError: Curl return value 35\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/abo/miniconda3/envs/CV/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/cfg/__init__.py\", line 943, in entrypoint\n",
            "    model = YOLO(model, task=task)\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/models/yolo/model.py\", line 23, in __init__\n",
            "    super().__init__(model=model, task=task, verbose=verbose)\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 146, in __init__\n",
            "    self._load(model, task=task)\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/engine/model.py\", line 289, in _load\n",
            "    self.model, self.ckpt = attempt_load_one_weight(weights)\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 910, in attempt_load_one_weight\n",
            "    ckpt, weight = torch_safe_load(weight)  # load ckpt\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/nn/tasks.py\", line 815, in torch_safe_load\n",
            "    file = attempt_download_asset(weight)  # search online if missing locally\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/utils/downloads.py\", line 454, in attempt_download_asset\n",
            "    safe_download(url=f\"{download_url}/{release}/{name}\", file=file, min_bytes=1e5, **kwargs)\n",
            "  File \"/home/abo/miniconda3/envs/CV/lib/python3.10/site-packages/ultralytics/utils/downloads.py\", line 364, in safe_download\n",
            "    raise ConnectionError(emojis(f\"❌  Download failure for {uri}. Retry limit reached.\")) from e\n",
            "ConnectionError: ❌  Download failure for https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt. Retry limit reached.\n"
          ]
        }
      ],
      "source": [
        "yolo_parameters = '''\n",
        "train: datset/train/images_and_detections\n",
        "val: datset/val/images_and_detections\n",
        "nc: 2\n",
        "names: ['no_tumor', 'tumor']\n",
        "'''\n",
        "with open(\"dataset/yolo_v8_parameters.yaml\", \"w\") as file:\n",
        "    file.write(yolo_parameters)\n",
        "\n",
        "!yolo train data=dataset/yolo_v8_parameters.yaml model=yolov8n.pt epochs=30 batch=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6jSy07Y-ln_"
      },
      "source": [
        "### **✅ Performance Yolo Model:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElwFUxOv_DGM"
      },
      "source": [
        "**Train Box Loss:**\n",
        "The train box loss metric measures the difference between the predicted bounding boxes and the actual bounding boxes of the objects in the training data. A lower box loss means that the model's predicted bounding boxes more closely align with the actual bounding boxes.\n",
        "\n",
        "**Train Class Loss:**\n",
        "The train class loss metric measures the difference between the predicted class probabilities and the actual class labels of the objects in the training data. A lower class loss means that the model's predicted class probabilities more closely align with the actual class labels.\n",
        "\n",
        "**Train DFL Loss:**\n",
        "The train DFL (Dynamic Feature Learning) loss metric measures the difference between the predicted feature maps and the actual feature maps of the objects in the training data. A lower DFL loss means that the model's predicted feature maps more closely align with the actual feature maps.\n",
        "\n",
        "**Metrics Precision (B):**\n",
        "The metrics precision (B) metric measures the proportion of true positive detections among all the predicted bounding boxes. A higher precision means that the model is better at correctly identifying true positive detections and minimizing false positives.\n",
        "\n",
        "**Metrics Recall (B):**\n",
        "The metrics recall (B) metric measures the proportion of true positive detections among all the actual bounding boxes. A higher recall means that the model is better at correctly identifying all true positive detections and minimizing false negatives.\n",
        "\n",
        "**Metrics mAP50 (B):**\n",
        "The metrics mAP50 (B) metric measures the mean average precision of the model across different object categories, with a 50% intersection-over-union (IoU) threshold. A higher mAP50 means that the model is better at accurately detecting and localizing objects across different categories.\n",
        "\n",
        "**Metrics mAP50-95 (B):**\n",
        "The metrics mAP50-95 (B) metric measures the mean average precision of the model across different object categories, with IoU thresholds ranging from 50% to 95%. A higher mAP50-95 means that the model is better at accurately detecting and localizing objects across different categories with a wider range of IoU thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPBhzun_MRL"
      },
      "outputs": [],
      "source": [
        "# Read in the results.csv file as a pandas dataframe\n",
        "df = pd.read_csv('/content/runs/detect/train/results.csv')\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Create subplots using seaborn (5 rows, 2 columns)\n",
        "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(15, 15))\n",
        "\n",
        "# Plot the columns using seaborn\n",
        "sns.lineplot(x='epoch', y='train/box_loss', data=df, ax=axes[0, 0])\n",
        "sns.lineplot(x='epoch', y='train/cls_loss', data=df, ax=axes[0, 1])\n",
        "sns.lineplot(x='epoch', y='train/dfl_loss', data=df, ax=axes[1, 0])\n",
        "sns.lineplot(x='epoch', y='metrics/precision(B)', data=df, ax=axes[1, 1])\n",
        "sns.lineplot(x='epoch', y='metrics/recall(B)', data=df, ax=axes[2, 0])\n",
        "sns.lineplot(x='epoch', y='metrics/mAP50(B)', data=df, ax=axes[2, 1])\n",
        "sns.lineplot(x='epoch', y='metrics/mAP50-95(B)', data=df, ax=axes[3, 0])\n",
        "sns.lineplot(x='epoch', y='val/box_loss', data=df, ax=axes[3, 1])\n",
        "sns.lineplot(x='epoch', y='val/cls_loss', data=df, ax=axes[4, 0])\n",
        "sns.lineplot(x='epoch', y='val/dfl_loss', data=df, ax=axes[4, 1])\n",
        "\n",
        "# Set titles and axis labels for each subplot\n",
        "axes[0, 0].set(title='Train Box Loss')\n",
        "axes[0, 1].set(title='Train Class Loss')\n",
        "axes[1, 0].set(title='Train DFL Loss')\n",
        "axes[1, 1].set(title='Metrics Precision (B)')\n",
        "axes[2, 0].set(title='Metrics Recall (B)')\n",
        "axes[2, 1].set(title='Metrics mAP50 (B)')\n",
        "axes[3, 0].set(title='Metrics mAP50-95 (B)')\n",
        "axes[3, 1].set(title='Validation Box Loss')\n",
        "axes[4, 0].set(title='Validation Class Loss')\n",
        "axes[4, 1].set(title='Validation DFL Loss')\n",
        "\n",
        "# Add suptitle and adjust layout\n",
        "plt.suptitle('Training Metrics and Loss', fontsize=24)\n",
        "plt.subplots_adjust(top=0.8)  # Adjust top margin to make space for suptitle\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHboePWaA7yA"
      },
      "source": [
        "### **✅ Evaluation Yolo Model:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zka0RTQyCOKl"
      },
      "source": [
        "**mAP Metrics:**\n",
        "Mean Average Precision (mAP) is a popular evaluation metric in object detection, including the YOLO model. It is used to assess the accuracy of an object detection model by measuring how well it can detect objects in an image, as well as the precision of those detections. mAP takes into account both the number of correctly identified objects and the quality of the detections, which means that it is a robust metric for assessing the performance of an object detection model.\n",
        "\n",
        "In YOLO, mAP is particularly important because it measures the accuracy of the model in detecting objects of interest. The higher the mAP, the better the model is at identifying objects in an image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypZyz-VkBDkH"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "metrics = model.val()\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KTZuX1wDBPk"
      },
      "outputs": [],
      "source": [
        "ax = sns.barplot(x=['mAP50-95', 'mAP50', 'mAP75'], y=[metrics.box.map, metrics.box.map50, metrics.box.map75])\n",
        "\n",
        "# Set the title and axis labels\n",
        "ax.set_title('YOLO Evaluation Metrics')\n",
        "ax.set_xlabel('Metric')\n",
        "ax.set_ylabel('Value')\n",
        "\n",
        "# Set the figure size\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(8, 6)\n",
        "\n",
        "# Add the values on top of the bars\n",
        "for p in ax.patches:\n",
        "    ax.annotate('{:.3f}'.format(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzW4JorBDwVc"
      },
      "outputs": [],
      "source": [
        "# Reading the confusion matrix image file\n",
        "img = mpimg.imread('/content/runs/detect/train/confusion_matrix.png')\n",
        "\n",
        "# Plotting the confusion matrix image\n",
        "fig, ax = plt.subplots(figsize = (15, 15))\n",
        "\n",
        "ax.imshow(img)\n",
        "ax.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0dYnKqiGJer"
      },
      "source": [
        "### **✅ Testing Yolo Model:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9GfgT1YHMXi"
      },
      "outputs": [],
      "source": [
        "# Load the trained YOLO model (replace with the actual path to your model)\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "\n",
        "# List of image paths to test (replace with your actual test image paths)\n",
        "image_paths = ['/content/LungTumorDetectionAndSegmentation/train/images/Subject_0/304.png', '/content/LungTumorDetectionAndSegmentation/train/images/Subject_0/310.png']\n",
        "\n",
        "# Perform inference on the images\n",
        "predictions = model.predict(image_paths, conf=0.25)\n",
        "\n",
        "# Visualizing and optionally saving the results\n",
        "for img_path, pred in zip(image_paths, predictions):\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    # Extract the bounding boxes and labels from the predictions\n",
        "    boxes = pred.boxes\n",
        "    detections = pred.names\n",
        "\n",
        "    # Draw the bounding boxes and labels on the image\n",
        "    for box, det in zip(boxes, detections):\n",
        "        x1, y1, x2, y2 = box.xyxy[0]\n",
        "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)  # Green box\n",
        "        cv2.putText(img, det, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Convert the image from BGR to RGB for displaying with matplotlib\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Display the image with predicted bounding boxes\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.axis('off')  # Hide axes\n",
        "    plt.show()\n",
        "\n",
        "    # Optionally, save the image with predictions\n",
        "    output_path = '/content/test_output/predicted_image.png'  # Define the output path for saving the image\n",
        "    cv2.imwrite(output_path, img)  # Save the image with bounding boxes\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pLgt2f4ByMt5",
        "eS_5IodBfU18",
        "0ptFwYHCkFlC"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
